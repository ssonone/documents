{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssonone/documents/blob/main/%5B%EC%8B%A4%EC%8A%B5%5D_11_(Colab)_%EC%98%A4%ED%94%88_%EB%AA%A8%EB%8D%B8%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_PEFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utEboyYtDGkg"
      },
      "source": [
        "# [실습] 오픈 모델을 이용한 PEFT\n",
        "\n",
        "허깅페이스의 모델을 다운로드하고,   \n",
        "데이터를 활용하여 LoRA 파인 튜닝을 수행합니다.   \n",
        "파인 튜닝의 목적은 **'건방진 QA 봇 만들기'** 입니다.\n",
        "\n",
        "### 중요) 코랩에서 실행하는 경우, T4 GPU 할당이 필요합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DRUQqC_DGkh"
      },
      "source": [
        "## 1. 실습 환경 세팅하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHX-RCPdkXgP"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KL0lKD0rM-1"
      },
      "outputs": [],
      "source": [
        "!pip install seaborn langchain langchain-huggingface transformers bitsandbytes pandas peft accelerate datasets huggingface_hub trl --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation\n",
        "# flash attention\n",
        "# Free Colab GPU: T4에서 적용 불가\n",
        "# Windows 설치 불가\n"
      ],
      "metadata": {
        "id": "-mwTwRvFFYoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28IjtEhiOi0a"
      },
      "source": [
        "# READ 토큰 불러오기\n",
        "\n",
        "https://huggingface.co/settings/tokens    \n",
        "위 링크에서 READ 권한 토큰을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tsVwizRu1kd"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# login(token='')\n",
        "# Read 권한 토큰을 입력하세요!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPDIKV6aBCCa"
      },
      "source": [
        "## 2.모델 찾아서 저장하고 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0Nl5mWL0k2T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
        "import transformers\n",
        "\n",
        "model_id='qwen/qwen2.5-3b-instruct' # 모델의 주소\n",
        "# huggingface.co/qwen/qwen2.5-3b-instruct\n",
        "\n",
        "\n",
        "\n",
        "# 양자화 Configuration 설정 - BitsAndBytes\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Ctc9MfOi0a"
      },
      "source": [
        "모델과 토크나이저를 불러옵니다.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHPq6J1dWe4v"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             torch_dtype='auto',\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             device_map={\"\":0},\n",
        "                                             # attn_implementation=\"flash_attention_2\"\n",
        "                                             # Free Colab GPU: T4에서 적용 불가\n",
        "                                             )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBdKC1z8PWZc"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "gen_config = dict(\n",
        "    do_sample=True,\n",
        "    max_new_tokens=512,\n",
        "    repetition_penalty = 1.1,\n",
        "    temperature = 0.7,\n",
        "    top_p = 0.8,\n",
        "    top_k = 20\n",
        ")\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, return_full_text=True,\n",
        "                **gen_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lnp8rsQG6Mc"
      },
      "source": [
        "## 파인 튜닝 데이터셋 준비하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9vYhL46G6Mc"
      },
      "source": [
        "SFT 실습에 사용할 데이터를 준비합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5Nu1SYwOi0b"
      },
      "outputs": [],
      "source": [
        "file_path = 'RAG_Data_full.csv'\n",
        "import pandas as pd\n",
        "pd.read_csv(file_path).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl1nNQGCG6Mg"
      },
      "source": [
        "datasets 라이브러리를 통해 데이터를 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm4FzCvfeYcK"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "file_path = ['RAG_Data_full.csv', 'RAG_Data_full_neg.csv']\n",
        "\n",
        "data = load_dataset(\"csv\",\n",
        "                    data_files={\"train\":file_path})\n",
        "\n",
        "# train_test split 나누기\n",
        "# data = load_dataset(\"csv\",\n",
        "#                     data_files={\"train\":file_path}, split='train').train_test_split(0.1)\n",
        "\n",
        "data = data.shuffle()\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5mW6bv3bxUG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiQx_JkcNyNv"
      },
      "source": [
        "데이터를 포맷팅하여 LLM에 입력할 프롬프트로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iv4TlzQHnG--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwv9ShYd3wM_"
      },
      "outputs": [],
      "source": [
        "def convert_format(context,question,answer=None, add_generation_prompt=False):\n",
        "    # add_generation_prompt의 필요 여부에 따라 결정\n",
        "\n",
        "    chat = [\n",
        "        {'role':'system',\n",
        "         'content':\"\"\"당신은 매우 거만합니다. [Context]를 참고하여, 사용자의 [Question]에 반말로 대답하세요.\n",
        "정답을 알고 있는 경우, 답변은 항상 '그것도 몰라?' 로 시작해야 합니다. 사용자의 질문에 대한 답변을 하기 위해 필요한 본문의 일부를 인용하세요.\n",
        "답을 모르는 경우, '내가 그딴 걸 어떻게 알아?' 라고만 답변하세요.\"\"\"},\n",
        "        {'role':'user',\n",
        "         'content':f\"Context: {context}\\n\"\n",
        "         \"---\\n\"\n",
        "        f\"Question: {question}\"}\n",
        "    ]\n",
        "    if answer:\n",
        "        chat.append(\n",
        "            {'role':'assistant',\n",
        "             'content':f\"{answer} END\"}\n",
        "        )\n",
        "    return {'text':tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=add_generation_prompt)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYI1ZEpCw0QA"
      },
      "outputs": [],
      "source": [
        "data = data.map(lambda x:convert_format(x['context'],x['question'], x['answer']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyIgZwb7lUOq"
      },
      "outputs": [],
      "source": [
        "[row for row in data['train']][0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWmCo87FG6Mh"
      },
      "outputs": [],
      "source": [
        "print(data['train'][0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_token_distribution(dataset, text_column='text', bins=30):\n",
        "\n",
        "    # 토큰 수 계산\n",
        "    token_counts = []\n",
        "    for text in dataset[text_column]:\n",
        "        tokens = tokenizer.encode(text)\n",
        "        token_counts.append(len(tokens))\n",
        "\n",
        "    # 기본 통계 계산\n",
        "    stats = {\n",
        "        '평균 토큰 수': np.mean(token_counts),\n",
        "        '중앙값': np.median(token_counts),\n",
        "        '최소 토큰 수': min(token_counts),\n",
        "        '최대 토큰 수': max(token_counts),\n",
        "        '표준편차': np.std(token_counts),\n",
        "        '90퍼센타일': np.percentile(token_counts, 90),\n",
        "        '95퍼센타일': np.percentile(token_counts, 95),\n",
        "        '99퍼센타일': np.percentile(token_counts, 99),\n",
        "        '총 샘플 수': len(token_counts)\n",
        "    }\n",
        "\n",
        "    # 분포 시각화\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # 히스토그램과 KDE\n",
        "    sns.histplot(data=token_counts, bins=bins, kde=True)\n",
        "    plt.title(f'Token Length Distribution for {text_column}')\n",
        "    plt.xlabel('Token Count')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 상세 통계 출력\n",
        "    print(\"\\n=== 토큰 수 통계 ===\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"{key}: {value:.1f}\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "print(analyze_token_distribution(data['train']))"
      ],
      "metadata": {
        "id": "NtWpu1NdJMlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxU6FA1XG6Mi"
      },
      "outputs": [],
      "source": [
        "test_context = '''OpenAI는 2018년에 최초의 GPT 모델(GPT-1)을 도입하여 \"생성 사전 훈련을 통한 언어 이해 개선\"이라는 논문을 발표했다.\n",
        "이는 트랜스포머 아키텍처를 기반으로 하며 대규모 책 모음에서 훈련되었다.\n",
        "다음 해에는 일관된 텍스트를 생성할 수 있는 더 큰 모델인 GPT-2를 도입했다.\n",
        "2020년에는 GPT-2보다 100배 많은 매개변수를 갖고 몇 가지 예제만으로 다양한 작업을 수행할 수 있는 모델인 GPT-3을 출시했다.\n",
        "GPT-3는 GPT-3.5로 더욱 개선되어 챗봇 제품인 ChatGPT를 만드는 데 사용되었다.\n",
        "소문에 따르면 GPT-4에는 1조 7600억 개의 매개변수가 있는데, 이는 실행 속도와 조지 호츠에 의해 처음 추정되었다.'''\n",
        "\n",
        "test_question = '''GPT-4의 파라미터는 몇 개인가요?'''\n",
        "\n",
        "test_prompt = convert_format(test_context,test_question, add_generation_prompt=True)['text']\n",
        "print(test_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgu1w4UdG6Mi"
      },
      "outputs": [],
      "source": [
        "response = pipe(test_prompt, truncation=True)\n",
        "\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp2gMi1ZzGET"
      },
      "source": [
        "## 6. PEFT(Prompt-Efficient Fine Tuning)로 학습하기   \n",
        "전체 파인 튜닝을 하지 않고도, PEFT를 사용하면 파라미터의 수를 줄인 효과적인 튜닝이 가능합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osPhWU2u0SYo"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaD3h0agDGkl"
      },
      "source": [
        "트랜스포머 기반의 모델이 아닌 경우, LoRa를 적용하는 레이어가 달라질 수 있습니다.   \n",
        "model의 출력 결과에서 모델의 구성 요소를 찾아봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ybeyl20n3dYH"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r = 4, # 4차원 (d-->r) : 보통은 8이나 16\n",
        "    lora_alpha=8, # (보통은 r과 함께 (8,16) 또는 (16,32))\n",
        "    target_modules=[\n",
        "    \"q_proj\",\n",
        "    \"k_proj\",\n",
        "    \"v_proj\",\n",
        "    \"o_proj\",\n",
        "    \"up_proj\",\n",
        "    \"down_proj\",\n",
        "    \"gate_proj\"],\n",
        "    # LoRA를 어떤 모듈에 부착할 것인가? (경험적)\n",
        "    # LoRA : Everything Except gate\n",
        "    # QLoRA(베이스모델을 양자화하는 경우): Everything\n",
        "\n",
        "    # Continuous Pretraining : embed_tokens 과 lm_head까지 부착 (메모리 소모 증가)\n",
        "    # Ref) https://unsloth.ai/blog/contpretraining\n",
        "\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu3qtdaVWYgE"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3jbNTs5nQX5"
      },
      "outputs": [],
      "source": [
        "# 학습되는 파라미터 수 출력\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0MOtwf3zdZp"
      },
      "source": [
        "Huggingface의 trl 라이브러리는 파인 튜닝을 쉽게 수행하게 해 주는 라이브러리입니다.     \n",
        "이번에 수행할 파인 튜닝은 Supervised Fine Tuning이므로, `SFTTrainer`를 불러와서 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwKCsG5wndlG"
      },
      "outputs": [],
      "source": [
        "data['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhArr94vbxUI"
      },
      "outputs": [],
      "source": [
        "len(data['train']['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1MKB9TmOi0d"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "response_template = \"<|im_start|>assistant\"\n",
        "# 답변부분만 학습시키기\n",
        "\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "# Data Collator : 데이터를 어떻게 전달할 것인가?\n",
        "# Next Token Prediction의 방법은 동일하나, 어디서부터 학습시킬 것인지를 결정\n",
        "# 질문 내용까지 학습하기 VS 답변만 학습하기\n",
        "# DataCollator를 전달하지 않으면, 전체 텍스트를 그대로 학습\n",
        "\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    report_to='none',\n",
        "\n",
        "    # evaluation_strategy=\"steps\",\n",
        "    # # logging_step당 validation loss 계산하는 옵션\n",
        "\n",
        "    max_steps= 500,\n",
        "    # batch의 입력을 max_steps만큼 수행\n",
        "    # batch * max_steps / len(data['train']) = Epochs\n",
        "    dataset_text_field=\"text\",\n",
        "    # dataset 'text' 필드를 사용\n",
        "\n",
        "    per_device_train_batch_size=1,\n",
        "    # GPU당 데이터 1개씩 입력\n",
        "\n",
        "    gradient_accumulation_steps=4,\n",
        "    # (Batch 대용) 그래디언트를 모아서 반영하는 스텝 수\n",
        "    # 배치사이즈를 키우는 것에 비해 메모리 소모가 감소하나, 속도가 느려짐\n",
        "\n",
        "    max_seq_length=768,\n",
        "    lr_scheduler_type='cosine',\n",
        "    # 학습률을 cosine 형태로 점진적 감소\n",
        "\n",
        "    learning_rate=1e-4,\n",
        "    bf16=True, # bfloat16 모델이므로 bf16 설정\n",
        "\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    output_dir=\"outputs\",\n",
        "    logging_steps=25,\n",
        "    # 손실함수 출력\n",
        "\n",
        "    # save_steps=50\n",
        "    # 체크포인트 저장\n",
        ")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=data['train'],\n",
        "    # # eval 데이터셋을 사용하고 싶은 경우 아래 주석 해제\n",
        "    # eval_dataset=data['test'],\n",
        "\n",
        "    args=sft_config,\n",
        "    data_collator=collator\n",
        "\n",
        ")\n",
        "\n",
        "with accelerator.main_process_first():\n",
        "    trainer.train()\n",
        "\n",
        "# Loss Function : Cross Entropy\n",
        "# 입력 데이터(배치)에 대한 평균 예측 확률 : e^(-Loss)\n",
        "# Ex) 0.2 --> 81% (e^-0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47U3zUpwG6Mj"
      },
      "source": [
        "## 7. 학습 결과 확인하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEZsPfYrbxUI"
      },
      "source": [
        "출력의 포맷을 END 토큰으로 끝맺었으므로, pipeline의 stop_sequence를 통해 출력을 제어합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_config = dict(\n",
        "    do_sample=True,\n",
        "    max_new_tokens=512,\n",
        "    repetition_penalty = 1.1,\n",
        "    temperature = 0.7,\n",
        "    top_p = 0.8,\n",
        "    top_k = 20\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, return_full_text=True,\n",
        "                **gen_config)"
      ],
      "metadata": {
        "id": "E9F-AWdt_oOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afMezykDG6Mj"
      },
      "outputs": [],
      "source": [
        "test_context = '''참고래의 등 부분은 밤회색이며, 배 쪽은 하얗다.\n",
        "튀어나온 두 쌍의 숨구멍이 있으며, 납작하고 넓은 주둥이를 가지고 있다.\n",
        "두 개의 밝은 색 문양이 숨구멍 뒤에서 시작해 몸의 측면으로 따라가 꼬리로 이어진다.\n",
        "오른쪽 턱에 하얀색 무늬가 있으며, 왼쪽은 회색 또는 검은색이다.\n",
        "참고래는 턱에서 몸 밑의 중앙부까지 이어지는 56에서 100개의 주름을 지니고 있는데,\n",
        "먹이를 잡을 때 목을 팽창시키기 쉽게 하기 위한 것이다.\n",
        "이들의 등지느러미의 길이는 60센티미터 정도이다.\n",
        "가슴지느러미는 아주 작으며, 꼬리는 넓고 V자 모양이며 끝은 뾰족한 편이다.'''\n",
        "\n",
        "test_question = '''참고래의 주름은 어떤 용도인가요?'''\n",
        "\n",
        "test_prompt = convert_format(test_context,test_question, add_generation_prompt=True)['text']\n",
        "\n",
        "print(test_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxDZD0zmbxUM"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "response = pipe(test_prompt, stop_sequence=\" END\")\n",
        "\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDrz9qcubxUM"
      },
      "outputs": [],
      "source": [
        "test_context = '''참고래의 등 부분은 밤회색이며, 배 쪽은 하얗다.\n",
        "튀어나온 두 쌍의 숨구멍이 있으며, 납작하고 넓은 주둥이를 가지고 있다.\n",
        "두 개의 밝은 색 문양이 숨구멍 뒤에서 시작해 몸의 측면으로 따라가 꼬리로 이어진다.\n",
        "오른쪽 턱에 하얀색 무늬가 있으며, 왼쪽은 회색 또는 검은색이다.\n",
        "참고래는 턱에서 몸 밑의 중앙부까지 이어지는 56에서 100개의 주름을 지니고 있는데,\n",
        "먹이를 잡을 때 목을 팽창시키기 쉽게 하기 위한 것이다.\n",
        "이들의 등지느러미의 길이는 60센티미터 정도이다.\n",
        "가슴지느러미는 아주 작으며, 꼬리는 넓고 V자 모양이며 끝은 뾰족한 편이다.'''\n",
        "\n",
        "test_question = '''참고래는 무슨 색인가요?'''\n",
        "\n",
        "test_prompt = convert_format(test_context,test_question, add_generation_prompt=True)['text']\n",
        "\n",
        "print(test_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-jauOEv9XVe"
      },
      "outputs": [],
      "source": [
        "response = pipe(test_prompt, stop_sequence=\" END\")\n",
        "\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvN9iBjA-x8V"
      },
      "source": [
        "## 8. 모델 저장하고 huggingface 계정에 업로드하기\n",
        "\n",
        "WRITE 권한 토큰이 필요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8UnErwL_AQ3"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "login('')\n",
        "\n",
        "# Write 권한 토큰 필요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEKOx3d0bxUM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K9oYY6MYq_d"
      },
      "outputs": [],
      "source": [
        "# model.save_pretrained('./Qwen2.5_Rude_RAG')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY0LMH2Q_Csw"
      },
      "outputs": [],
      "source": [
        "# # # 개인 계정 주소에 업로드하기\n",
        "\n",
        "# model.push_to_hub('NotoriousH2/Qwen2.5_Rude_RAG')\n",
        "# tokenizer.push_to_hub('NotoriousH2/Qwen2.5_Rude_RAG')\n",
        "# # # 토크나이저도 함께 업로드\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNJU0hqHbxUN"
      },
      "source": [
        "만약 베이스모델이 포함된 전체 모델을 업로드하고 싶다면 `merge_and_unload()`를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcizYkEua8QR"
      },
      "outputs": [],
      "source": [
        "model = model.merge_and_unload()\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7psjpd0bxUN"
      },
      "outputs": [],
      "source": [
        "# # # 개인 계정 주소에 업로드하기\n",
        "\n",
        "# model.push_to_hub('NotoriousH2/Qwen2.5_Rude_RAG_FULL')\n",
        "# tokenizer.push_to_hub('NotoriousH2/Qwen2.5_Rude_RAG_FULL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zni2JLSbxUN"
      },
      "outputs": [],
      "source": [
        "# # 이후 작업을 위해 오프라인 저장\n",
        "# model.save_pretrained('./Qwen2_Rude_RAG_FULL')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrIeUEb3bxUN"
      },
      "source": [
        "모델을 저장한 뒤, Kernel Restart를 실행하여 공간을 비웁니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "anvoekOZ_Yq4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}